{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Just Do It\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\JUSTDO~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.856 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12281458  0.36768654]\n"
     ]
    }
   ],
   "source": [
    "# 建立语料特征，模型(TFIDF),相似度\n",
    "from gensim import corpora,models,similarities\n",
    "import jieba\n",
    "import urllib.request\n",
    "# 解决编码问题从页面获取\n",
    "# 老九门\n",
    "# 鬼吹灯\n",
    "with open('./老九门.txt', 'r', encoding = 'utf-8') as f_:\n",
    "    d1 = f_.read()\n",
    "with open('./鬼吹灯.txt', 'r', encoding = 'utf-8') as f_2:\n",
    "    d2 = f_2.read()\n",
    "data1=jieba.cut(d1)\n",
    "data2=jieba.cut(d2)\n",
    "# 处理格式\n",
    "# \"词语1 词语2 词语3 … 词语n\"\n",
    "data11=\"\"\n",
    "for item in data1:\n",
    "    data11+=item+\" \"\n",
    "data21=\"\"\n",
    "for item in data2:\n",
    "    data21+=item+\" \"\n",
    "documents=[data11,data21]\n",
    "texts=[[word for word in document.split()] for document in documents]\n",
    "\n",
    "# 频率 frequency\n",
    "from collections import defaultdict\n",
    "frequency=defaultdict(int)\n",
    "for text in texts:\n",
    "\tfor token in text:\n",
    "    # 计算频率\n",
    "\t\tfrequency[token]+=1\n",
    "# 将词频低的词语过滤掉\n",
    "texts=[[token for token in text if frequency[token]>25] for text in texts]\n",
    "\n",
    "# 建立语料特征词典\n",
    "dictionary=corpora.Dictionary(texts)\n",
    "# dictionary.save('./12345.txt') \n",
    "# doc3=\"D:/Python35/d3.txt\"\n",
    "# 盗墓笔记\n",
    "with open('./盗墓笔记.TXT', 'r', encoding = 'utf-8') as f_1:\n",
    "    d3 = f_1.read()\n",
    "data3=jieba.cut(d3)\n",
    "data31=\"\"\n",
    "for item in data3:\n",
    "    data31+=item+\" \"\n",
    "new_doc=data31\n",
    "# 将要对比的文档通过doc2bow转化为稀疏向量\n",
    "new_vec=dictionary.doc2bow(new_doc.split())\n",
    "# 建立新的语料库\n",
    "# 对稀疏向量进行进一步处理，得到新语料库\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "# 保存稀疏向量语料库\n",
    "# corpora.MmCorpus.serialize('./6562.txt',corpus) \n",
    "# 将新语料库通过tfidfmodel进行处理，得到tfidf\n",
    "tfidf=models.TfidfModel(corpus)\n",
    "# 通过token2id得到特征数\n",
    "featureNum=len(dictionary.token2id.keys())\n",
    "# 稀疏矩阵相似度， 语料，特征数\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=featureNum)\n",
    "# 建立索引, 计算相似度\n",
    "sims=index[tfidf[new_vec]]\n",
    "# 大于30%，相似度就很高了\n",
    "print(sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好好', '好好学', '好好学习', '好学', '学习', '', '', '天天', '天天向上', '向上', '', '', '少', '打游戏', '游戏', '', '', '多', '看点', '书']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "# 全模式分词\n",
    "seg_list = jieba.cut('好好学习，天天向上，少打游戏，多看点书', cut_all=True)\n",
    "print(list(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精准模式(默认)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好好学习', '，', '天天向上', '，', '少', '打游戏', '，', '多', '看点', '书']\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('好好学习，天天向上，少打游戏，多看点书', cut_all=False)\n",
    "print(list(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搜索引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['好好', '好学', '学习', '好好学', '好好学习', '，', '天天', '向上', '天天向上', '，', '少', '游戏', '打游戏', '，', '多', '看点', '书']\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search('好好学习，天天向上，少打游戏，多看点书')\n",
    "print(list(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 返回list 的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "干森 么 了 ， 你 很 奇怪 耶 ， 整天 就 知道 吃 ,   吃饱 了 就 睡觉\n",
      "干森 么 了 ， 你 很 奇怪 耶 ， 整天 就 知道 吃 ， 吃饱 了 就 睡觉\n"
     ]
    }
   ],
   "source": [
    "result_lcut = jieba.lcut('干森么了，你很奇怪耶，整天就知道吃, 吃饱了就睡觉')\n",
    "print(' '.join(result_lcut))\n",
    "print(' '.join(jieba.lcut_for_search('干森么了，你很奇怪耶，整天就知道吃，吃饱了就睡觉')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "干森 么 了 ， 你 很 奇怪 耶 ， 整天 就 知道 吃 ,   吃饱了 就 睡觉\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('./self.txt')\n",
    "print(' '.join(jieba.lcut('干森么了，你很奇怪耶，整天就知道吃, 吃饱了就睡觉')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义调节单个词语的词频，使其不能或者能被分出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "干 森 么 了 ， 你 很 奇 怪 耶 ， 整天 就 知道 吃 ,   吃饱了 就 睡觉\n"
     ]
    }
   ],
   "source": [
    "jieba.suggest_freq(('干', '森'), True)\n",
    "print(' '.join(jieba.lcut('干森么了，你很奇怪耶，整天就知道吃, 吃饱了就睡觉')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut('你知不知道你很奇怪耶')\n",
    "print(dict(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行分词(参考的demo，未验证)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows\n",
    "\n",
    "> 用法：\n",
    "\n",
    ">jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数\n",
    ">jieba.disable_parallel() # 关闭并行分词模式\n",
    ">实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。\n",
    "\n",
    "> 注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import jieba\n",
    "\n",
    "jieba.enable_parallel()\n",
    "content = open(u'西游记.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('并行分词速度为 %s bytes/second' % (len(content)/tm_cost))\n",
    "\n",
    "jieba.disable_parallel()\n",
    "content = open(u'西游记.txt',\"r\").read()\n",
    "t1 = time.time()\n",
    "words = \"/ \".join(jieba.cut(content))\n",
    "t2 = time.time()\n",
    "tm_cost = t2-t1\n",
    "print('非并行分词速度为 %s bytes/second' % (len(content)/tm_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 并行分词速度为 830619.50933 bytes/second\n",
    "\n",
    "> 非并行分词速度为 259941.448353 bytes/second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize -> 返回词语在原文的起止位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> 输入参数只接受 unicode （python3 所有的字符串都被视为Unicode，写u'xx'和'xx'）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
